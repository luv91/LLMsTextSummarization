[2023-12-29 16:14:57,838: INFO: main: Welcome to out custom logging]
[2023-12-29 16:15:35,433: INFO: main: Welcome to out custom logging]
[2023-12-29 19:37:10,553: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-29 19:39:42,615: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-29 19:39:42,618: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-29 19:39:42,619: INFO: common: created directory at: artifacts]
[2023-12-29 19:39:42,619: INFO: common: created directory at: artifacts/data_ingestion]
[2023-12-29 19:39:43,309: INFO: 2896332844: artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 1F60:76A0:91B68C:AF3CA4:658F66CD
Accept-Ranges: bytes
Date: Sat, 30 Dec 2023 00:39:43 GMT
Via: 1.1 varnish
X-Served-By: cache-bos4651-BOS
X-Cache: MISS
X-Cache-Hits: 0
X-Timer: S1703896783.060074,VS0,VE167
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: 9c8859295404abb0be0ccc5d651d433bb72ce2b5
Expires: Sat, 30 Dec 2023 00:44:43 GMT
Source-Age: 0

]
[2023-12-29 20:00:19,515: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2023-12-29 20:00:19,516: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-29 20:00:19,517: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-29 20:00:19,517: INFO: common: created directory at: artifacts]
[2023-12-29 20:00:19,517: INFO: common: created directory at: artifacts/data_ingestion]
[2023-12-29 20:00:19,987: INFO: data_ingestion: artifacts/data_ingestion/data.zip download! with following info: 
Connection: close
Content-Length: 7903594
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: application/zip
ETag: "dbc016a060da18070593b83afff580c9b300f0b6ea4147a7988433e04df246ca"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: 1F60:76A0:91B68C:AF3CA4:658F66CD
Accept-Ranges: bytes
Date: Sat, 30 Dec 2023 01:00:19 GMT
Via: 1.1 varnish
X-Served-By: cache-bos4649-BOS
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1703898020.812928,VS0,VE70
Vary: Authorization,Accept-Encoding,Origin
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: d66ada2f5325958d2ba2f5bedeb7686bea92731b
Expires: Sat, 30 Dec 2023 01:05:19 GMT
Source-Age: 0

]
[2023-12-29 20:00:20,095: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2023-12-30 00:13:06,583: INFO: config: PyTorch version 2.1.2 available.]
[2023-12-30 00:13:07,568: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2023-12-30 00:13:07,570: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-30 00:13:07,571: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-30 00:13:07,571: INFO: common: created directory at: artifacts]
[2023-12-30 00:13:07,571: INFO: common: created directory at: artifacts/data_ingestion]
[2023-12-30 00:13:07,572: INFO: data_ingestion: File already exists of size: ~ 7718 KB]
[2023-12-30 00:13:07,680: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2023-12-30 00:13:07,681: INFO: main: >>>>>> stage Data Validation stage started <<<<<<]
[2023-12-30 00:13:07,683: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-30 00:13:07,684: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-30 00:13:07,684: INFO: common: created directory at: artifacts]
[2023-12-30 00:13:07,684: INFO: common: created directory at: artifacts/data_validation]
[2023-12-30 00:13:07,685: INFO: main: >>>>>> stage Data Validation stage completed <<<<<<

x==========x]
[2023-12-30 00:13:07,685: INFO: main: >>>>>> stage Data Transformation stage started <<<<<<]
[2023-12-30 00:13:07,687: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-30 00:13:07,688: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-30 00:13:07,688: INFO: common: created directory at: artifacts]
[2023-12-30 00:13:07,688: INFO: common: created directory at: artifacts/data_transformation]
[2023-12-30 00:13:10,443: INFO: main: >>>>>> stage Data Transformation stage completed <<<<<<

x==========x]
[2023-12-30 00:13:10,443: INFO: main: *******************]
[2023-12-30 00:13:10,443: INFO: main: >>>>>> stage Model Trainer stage started <<<<<<]
[2023-12-30 00:13:10,445: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-30 00:13:10,446: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-30 00:13:10,446: INFO: common: created directory at: artifacts]
[2023-12-30 00:13:10,446: INFO: common: created directory at: artifacts/model_trainer]
[2023-12-30 00:14:25,346: ERROR: main: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`]
Traceback (most recent call last):
  File "/Users/luv/Documents/LLMs/LLMsTextSummarization/main.py", line 47, in <module>
    model_trainer.main()
  File "/Users/luv/Documents/LLMs/LLMsTextSummarization/src/TextSummarization/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
  File "/Users/luv/Documents/LLMs/LLMsTextSummarization/src/TextSummarization/components/model_trainer.py", line 32, in train
    trainer_args = TrainingArguments(
  File "<string>", line 121, in __init__
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/transformers/training_args.py", line 1493, in __post_init__
    and (self.device.type != "cuda")
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/transformers/training_args.py", line 1941, in device
    return self._setup_devices
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/transformers/training_args.py", line 1841, in _setup_devices
    raise ImportError(
ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`
[2023-12-30 00:14:58,713: INFO: config: PyTorch version 2.1.2 available.]
[2023-12-30 00:14:59,483: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2023-12-30 00:14:59,486: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-30 00:14:59,487: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-30 00:14:59,487: INFO: common: created directory at: artifacts]
[2023-12-30 00:14:59,487: INFO: common: created directory at: artifacts/data_ingestion]
[2023-12-30 00:14:59,487: INFO: data_ingestion: File already exists of size: ~ 7718 KB]
[2023-12-30 00:14:59,597: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2023-12-30 00:14:59,597: INFO: main: >>>>>> stage Data Validation stage started <<<<<<]
[2023-12-30 00:14:59,600: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-30 00:14:59,600: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-30 00:14:59,601: INFO: common: created directory at: artifacts]
[2023-12-30 00:14:59,601: INFO: common: created directory at: artifacts/data_validation]
[2023-12-30 00:14:59,602: INFO: main: >>>>>> stage Data Validation stage completed <<<<<<

x==========x]
[2023-12-30 00:14:59,602: INFO: main: >>>>>> stage Data Transformation stage started <<<<<<]
[2023-12-30 00:14:59,604: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-30 00:14:59,605: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-30 00:14:59,605: INFO: common: created directory at: artifacts]
[2023-12-30 00:14:59,605: INFO: common: created directory at: artifacts/data_transformation]
[2023-12-30 00:15:00,403: INFO: main: >>>>>> stage Data Transformation stage completed <<<<<<

x==========x]
[2023-12-30 00:15:00,404: INFO: main: *******************]
[2023-12-30 00:15:00,404: INFO: main: >>>>>> stage Model Trainer stage started <<<<<<]
[2023-12-30 00:15:00,406: INFO: common: yaml file: config/config.yaml loaded successfully]
[2023-12-30 00:15:00,407: INFO: common: yaml file: params.yaml loaded successfully]
[2023-12-30 00:15:00,407: INFO: common: created directory at: artifacts]
[2023-12-30 00:15:00,407: INFO: common: created directory at: artifacts/model_trainer]
[2023-12-30 00:16:59,122: ERROR: main: MPS backend out of memory (MPS allocated: 8.15 GB, other allocations: 9.97 GB, max allowed: 18.13 GB). Tried to allocate 16.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).]
Traceback (most recent call last):
  File "/Users/luv/Documents/LLMs/LLMsTextSummarization/main.py", line 47, in <module>
    model_trainer.main()
  File "/Users/luv/Documents/LLMs/LLMsTextSummarization/src/TextSummarization/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
  File "/Users/luv/Documents/LLMs/LLMsTextSummarization/src/TextSummarization/components/model_trainer.py", line 45, in train
    trainer.train()
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/transformers/trainer.py", line 1902, in _inner_training_loop
    self.optimizer.step()
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/accelerate/optimizer.py", line 145, in step
    self.optimizer.step(closure)
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/torch/optim/adamw.py", line 173, in step
    self._init_group(
  File "/Users/luv/miniconda3/envs/LLMTextSummarization_env/lib/python3.9/site-packages/torch/optim/adamw.py", line 125, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
RuntimeError: MPS backend out of memory (MPS allocated: 8.15 GB, other allocations: 9.97 GB, max allowed: 18.13 GB). Tried to allocate 16.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
